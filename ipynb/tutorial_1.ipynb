{
 "metadata": {
  "name": "tutorial_1"
 },
 "nbformat": 3,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "source": [
      "Gensim Tutorial 1: Corpora and Vector Spaces"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "This is [part 1](http://radimrehurek.com/gensim/tut1.html) of the Gensim tutorial."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "From Strings to Vectors"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "This time, let\u2019s start from documents represented as strings:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import logging",
      "from pprint import pprint",
      "from gensim import corpora",
      "",
      "documents = [\"Human machine interface for lab abc computer applications\",",
      "             \"A survey of user opinion of computer system response time\",",
      "             \"The EPS user interface management system\",",
      "             \"System and human system engineering testing of EPS\",",
      "             \"Relation of user perceived response time to error measurement\",",
      "             \"The generation of random binary unordered trees\",",
      "             \"The intersection graph of paths in trees\",",
      "             \"Graph minors IV Widths of trees and well quasi ordering\",",
      "             \"Graph minors A survey\"]"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "This is a tiny corpus of nine documents, each consisting of only a single sentence.",
      "",
      "First, let\u2019s tokenize the documents, remove common words (using a toy stoplist) as well as words that only appear once in the corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = set('for a of the and to in'.split())",
      "texts = [[word for word in document.lower().split() if word not in stoplist]",
      "          for document in documents]",
      "",
      "# remove words that appear only once",
      "all_tokens = sum(texts, [])",
      "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)",
      "texts = [[word for word in text if word not in tokens_once]",
      "          for text in texts]",
      "",
      "pprint(texts)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['human', 'interface', 'computer'],",
        " ['survey', 'user', 'computer', 'system', 'response', 'time'],",
        " ['eps', 'user', 'interface', 'system'],",
        " ['system', 'human', 'system', 'eps'],",
        " ['user', 'response', 'time'],",
        " ['trees'],",
        " ['graph', 'trees'],",
        " ['graph', 'minors', 'trees'],",
        " ['graph', 'minors', 'survey']]",
        ""
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "source": [
      "Your way of processing the documents will likely vary; here, I only split on whitespace to tokenize, followed by lowercasing each word. In fact, I use this particular (simplistic and inefficient) setup to mimick the experiment done in Deerwester et al.\u2019s original LSA article [1].",
      "",
      "The ways to process documents are so varied and application- and language-dependent that I decided to not constrain them by any interface. Instead, a document is represented by the features extracted from it, not by its \u201csurface\u201d string form: how you get to the features is up to you. Below I describe one common, general-purpose approach (called bag-of-words), but keep in mind that different application domains call for different features, and, as always, it\u2019s garbage in, garbage out...",
      "",
      "To convert documents to vectors, we\u2019ll use a document representation called bag-of-words. In this representation, each document is represented by one vector where each vector element represents a question-answer pair, in the style of:",
      "",
      "\u201cHow many times does the word system appear in the document? Once.\u201d",
      "It is advantageous to represent the questions only by their (integer) ids. The mapping between the questions and ids is called a dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(texts)",
      "dictionary.save('/tmp/deerwester.dict') # store the dictionary, for future reference",
      "print dictionary"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(12 unique tokens)",
        ""
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "Here we assigned a unique integer id to all words appearing in the corpus with the gensim.corpora.dictionary.Dictionary class. This sweeps across the texts, collecting word counts and relevant statistics. In the end, we see there are twelve distinct words in the processed corpus, which means each document will be represented by twelve numbers (ie., by a 12-D vector). To see the mapping between words and their ids:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint(dictionary.token2id)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'computer': 0,",
        " 'eps': 8,",
        " 'graph': 10,",
        " 'human': 1,",
        " 'interface': 2,",
        " 'minors': 11,",
        " 'response': 3,",
        " 'survey': 4,",
        " 'system': 5,",
        " 'time': 6,",
        " 'trees': 9,",
        " 'user': 7}",
        ""
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "source": [
      "To actually convert tokenized documents to vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_doc = \"Human computer interaction\"",
      "new_vec = dictionary.doc2bow(new_doc.lower().split())",
      "pprint(new_vec) # the word \"interaction\" does not appear in the dictionary and is ignored"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 1), (1, 1)]",
        ""
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "source": [
      "The function `doc2bow()` simply counts the number of occurences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. The sparse vector `[(0, 1), (1, 1)]` therefore reads: in the document `\u201cHuman computer interaction\u201d`, the words `computer` (id 0) and `human` (id 1) appear once; the other ten dictionary words appear (implicitly) zero times.",
      ""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [dictionary.doc2bow(text) for text in texts]",
      "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use",
      "pprint(corpus)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(0, 1), (1, 1), (2, 1)],",
        " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],",
        " [(2, 1), (5, 1), (7, 1), (8, 1)],",
        " [(1, 1), (5, 2), (8, 1)],",
        " [(3, 1), (6, 1), (7, 1)],",
        " [(9, 1)],",
        " [(9, 1), (10, 1)],",
        " [(9, 1), (10, 1), (11, 1)],",
        " [(4, 1), (10, 1), (11, 1)]]",
        ""
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Corpus Streaming \u2013 One Document at a Time"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Note that `corpus` above resides fully in memory, as a plain Python list. In this simple example, it doesn\u2019t matter much, but just to make things clear, let\u2019s assume there are millions of documents in the corpus. Storing all of them in RAM won\u2019t do. Instead, let\u2019s assume the documents are stored in a file on disk, one document per line. Gensim only requires that a corpus must be able to return one document vector at a time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class MyCorpus(object):",
      "    def __iter__(self):",
      "        for line in open('mycorpus.txt'):",
      "            # assume there's one document per line, tokens separated by whitespace",
      "            yield dictionary.doc2bow(line.lower().split())"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "source": [
      "Download the sample [mycorpus.txt file here](http://radimrehurek.com/gensim/mycorpus.txt). The assumption that each document occupies one line in a single file is not important; you can mold the `__iter__` function to fit your input format, whatever it is. Walking directories, parsing XML, accessing network, etc. Just parse your input to retrieve a clean list of tokens in each document, then convert the tokens via a dictionary to their ids and yield the resulting sparse vector inside `__iter__`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!",
      "print corpus_memory_friendly"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<__main__.MyCorpus object at 0x9f5e14c>",
        ""
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "source": [
      "Corpus is now an object. We didn\u2019t define any way to print it, so `print` just outputs address of the object in memory. Not very useful. To see the constituent vectors, let\u2019s iterate over the corpus and print each document vector (one at a time):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for vector in corpus_memory_friendly: # load one vector into memory at a time",
      "    print vector"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 1), (1, 1), (2, 1)]",
        "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]",
        "[(2, 1), (5, 1), (7, 1), (8, 1)]",
        "[(1, 1), (5, 2), (8, 1)]",
        "[(3, 1), (6, 1), (7, 1)]",
        "[(9, 1)]",
        "[(9, 1), (10, 1)]",
        "[(9, 1), (10, 1), (11, 1)]",
        "[(4, 1), (10, 1), (11, 1)]",
        ""
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "source": [
      "Although the output is the same as for the plain Python list, the corpus is now much more memory friendly, because at most one vector resides in RAM at a time. Your corpus can now be as large as you want.",
      "",
      "Similarly, to construct the dictionary without loading all texts into memory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect statistics about all tokens",
      "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))",
      "# remove stop words and words that appear only once",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist",
      "            if stopword in dictionary.token2id]",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]",
      "dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once",
      "dictionary.compactify() # remove gaps in id sequence after words that were removed",
      "print dictionary"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(12 unique tokens)",
        ""
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "source": [
      "And that is all there is to it! At least as far as bag-of-words representation is concerned. Of course, what we do with such corpus is another question; it is not at all clear how counting the frequency of distinct words could be useful. As it turns out, it isn\u2019t, and we will need to apply a transformation on this simple representation first, before we can use it to compute any meaningful document vs. document similarities. Transformations are covered in the next tutorial, but before that, let\u2019s briefly turn our attention to corpus persistency."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Corpus Formats"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "There exist several file formats for serializing a Vector Space corpus (~sequence of vectors) to disk. `Gensim` implements them via the streaming corpus interface mentioned earlier: documents are read from (resp. stored to) disk in a lazy fashion, one document at a time, without the whole corpus being read into main memory at once.",
      "",
      "One of the more notable file formats is the Market Matrix format. To save a corpus in the Matrix Market format:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from gensim import corpora",
      "# create a toy corpus of 2 documents, as a plain Python list",
      "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it",
      "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "source": [
      "Other formats include Joachim\u2019s SVMlight format, Blei\u2019s LDA-C format and GibbsLDA++ format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)",
      "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)",
      "corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "source": [
      "Conversely, to load a corpus iterator from a Matrix Market file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "corpus = corpora.MmCorpus('/tmp/corpus.mm')"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print corpus",
      ""
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(2 documents, 2 features, 1 non-zero entries)",
        ""
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "source": [
      "Instead, to view the contents of a corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# one way of printing a corpus: load it entirely into memory",
      "print list(corpus) # calling list() will convert any sequence to a plain Python list"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(1, 0.5)], []]",
        ""
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "source": [
      "or"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# another way of doing it: print one document at a time, making use of the streaming interface",
      "for doc in corpus:",
      "    print doc"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(1, 0.5)]",
        "[]",
        ""
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "source": [
      "The second way is obviously more memory-friendly, but for testing and development purposes, nothing beats the simplicity of calling `list(corpus)`.",
      "",
      "To save the same Matrix Market document stream in Blei\u2019s LDA-C format,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "source": [
      "In this way, `gensim` can also be used as a memory-efficient I/O format conversion tool: just load a document stream using one format and immediately save it in another format. Adding new formats is dead easy, check out the code for the SVMlight corpus for an example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      ""
     ],
     "language": "python",
     "outputs": []
    }
   ]
  }
 ]
}