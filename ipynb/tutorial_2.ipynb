{
 "metadata": {
  "name": "tutorial_2"
 },
 "nbformat": 3,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "source": [
      "Topics and Transformations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Transformation interface"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "In the previous tutorial on Corpora and Vector Spaces, we created a corpus of documents represented as a stream of vectors. To continue, let\u2019s fire up `gensim` and use that corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities",
      "dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')",
      "corpus = corpora.MmCorpus('/tmp/deerwester.mm')",
      "print corpus"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(9 documents, 12 features, 28 non-zero entries)",
        ""
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "source": [
      "In this tutorial, I will show how to transform documents from one vector representation into another. This process serves two goals:",
      "",
      "1. To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.",
      "",
      "2. To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy (marginal data trends are ignored, noise-reduction)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Creating a transformation"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "The transformations are standard Python objects, typically initialized by means of a training corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "We used our old corpus from tutorial 1 to initialize (train) the transformation model. Different transformations may require different initialization parameters; in case of TfIdf, the \u201ctraining\u201d consists simply of going through the supplied corpus once and computing document frequencies of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involved and, consequently, takes much more time."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "###Note",
      "",
      "> Transformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Transforming Vectors"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "From now on, `tfidf` is treated as a read-only object that can be used to convert any vector from the old representation (bag-of-words integer counts) to the new representation (TfIdf real-valued weights):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_bow = [(0, 1), (1, 1)]",
      "print tfidf[doc_bow] # step 2 -- use the model to transform vectors"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.7071067811865476), (1, 0.7071067811865476)]",
        ""
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "source": [
      "Or to apply a transformation to a whole corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_tfidf = tfidf[corpus]",
      "for doc in corpus_tfidf:",
      "    print doc"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]",
        "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]",
        "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]",
        "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]",
        "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]",
        "[(9, 1.0)]",
        "[(9, 0.7071067811865475), (10, 0.7071067811865475)]",
        "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]",
        "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]",
        ""
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "source": [
      "In this particular case, we are transforming the same corpus that we used for training, but this is only incidental. Once the transformation model has been initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used in the training corpus at all. This is achieved by a process called folding-in for LSA, by topic inference for LDA etc."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "### Note",
      "",
      "> Calling `model[corpus]` only creates a wrapper around the old `corpus` document stream \u2013 actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling `corpus_transformed = model[corpus]`, because that would mean storing the result in main memory, and that contradicts gensim\u2019s objective of memory-indepedence. If you will be iterating over the transformed `corpus_transformed` multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Transformations can also be serialized, one on top of another, in a sort of chain:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation",
      "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "source": [
      "Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set `num_topics=2`). Now you\u2019re probably wondering: what do these two latent dimensions stand for? Let\u2019s inspect with `models.LsiModel.print_topics()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(2)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "source": [
      "It appears that according to LSI, \u201ctrees\u201d, \u201cgraph\u201d and \u201cminors\u201d are all related words (and contribute the most to the direction of the first topic), while the second topic practically concerns itself with all the other words. As expected, the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly",
      "    print doc"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.066007833960903012), (1, -0.52007033063618469)]",
        "[(0, 0.19667592859142458), (1, -0.76095631677000519)]",
        "[(0, 0.08992639972446359), (1, -0.72418606267525087)]",
        "[(0, 0.07585847652178096), (1, -0.63205515860034256)]",
        "[(0, 0.10150299184980106), (1, -0.57373084830029608)]",
        "[(0, 0.70321089393783132), (1, 0.16115180214025734)]",
        "[(0, 0.87747876731198338), (1, 0.16758906864659337)]",
        "[(0, 0.90986246868185805), (1, 0.14086553628718931)]",
        "[(0, 0.61658253505692817), (1, -0.053929075663894384)]",
        ""
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "source": [
      "Model persistency is achieved with the save() and load() functions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "lsi.save('/tmp/model.lsi') # same for tfidf, lda, ...",
      "lsi = models.LsiModel.load('/tmp/model.lsi')"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "source": [
      "The next question might be: just how exactly similar are those documents to each other? Is there a way to formalize the similarity, so that for a given input document, we can order some other set of documents according to their similarity? Similarity queries are covered in the next tutorial."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Gensim implements several popular Vector Space Model algorithms:",
      "",
      "* Term Frequency * Inverse Document Frequency, Tf-Idf expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality, except that features which were rare in the training corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving the number of dimensions intact. It can also optionally normalize the resulting vectors to (Euclidean) unit length."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model = models.tfidfmodel.TfidfModel(bow_corpus, normalize=True)"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "* Latent Semantic Indexing, LSI (or sometimes LSA) transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into a latent space of a lower dimensionality. For the toy corpus above we used only 2 latent dimensions, but on real corpora, target dimensionality of 200\u2013500 is recommended as a \u201cgolden standard\u201d [1]."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model = lsimodel.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)",
      "",
      "LSI training is unique in that we can continue \u201ctraining\u201d at any point, simply by providing more training documents. This is done by incremental updates to the underlying model, in a process called online training. Because of this feature, the input document stream may even be infinite \u2013 just keep feeding LSI new documents as they arrive, while using the computed transformation model as read-only in the meanwhile!",
      ""
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model.add_documents(another_tfidf_corpus) # now LSI has been trained on tfidf_corpus another_tfidf_corpus",
      "    lsi_vec = model[tfidf_vec] # convert some new document into the LSI space, without affecting the model",
      "    ...",
      "    model.add_documents(more_documents) # tfidf_corpus + another_tfidf_corpus + more_documents",
      "    lsi_vec = model[tfidf_vec]"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "See the gensim.models.lsimodel documentation for details on how to make LSI gradually \u201cforget\u201d old observations in infinite streams. If you want to get dirty, there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical precision of the LSI algorithm.",
      "",
      "`gensim` uses a novel online incremental streamed distributed training algorithm (quite a mouthful!), which I published in [5]. `gensim` also executes a stochastic multi-pass algorithm from Halko et al. [4] internally, to accelerate in-core part of the computations. See also Experiments on the English Wikipedia for further speed-ups by distributing the computation across a cluster of computers.",
      "",
      "* Random Projections, RP aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness. Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model = rpmodel.RpModel(tfidf_corpus, num_topics=500)"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "* Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA\u2019s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model = ldamodel.LdaModel(bow_corpus, id2word=dictionary, num_topics=100)",
      "",
      "`gensim` uses a fast implementation of online LDA parameter estimation based on [2], modified to run in distributed mode on a cluster of computers."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "* Hierarchical Dirichlet Process, HDP is a non-parametric bayesian method (note the missing number of requested topics):"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "    model = hdpmodel.HdpModel(bow_corpus, id2word=dictionary)",
      "",
      "`gensim` uses a fast, online implementation based on [3]. The HDP model is a new addition to `gensim`, and still rough around its academic edges \u2013 use with care."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "It is worth repeating that these are all unique, incremental implementations, which do not require the whole training corpus to be present in main memory all at once. With memory taken care of, I am now improving Distributed Computing, to improve CPU efficiency, too. If you feel you could contribute (by testing, providing use-cases or code), please let me know.",
      "",
      "Continue on to the next tutorial on Similarity Queries."
     ]
    }
   ]
  }
 ]
}